{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# get top3 players from each team\n",
    "\n",
    "teams = ['ATL', 'BOS', 'BRK', 'CHI', 'CHO', 'CLE', 'DAL', 'DEN', 'DET', 'GSW', 'HOU', 'IND', 'LAC', 'LAL', 'MEM', 'MIA', 'MIL', 'MIN', 'NOP', 'NYK', 'OKC', 'ORL', 'PHI', 'PHO', 'POR', 'SAC', 'SAS', 'TOR', 'UTA', 'WAS']\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/t55f5t150hv1sqqwhrm_md340000gn/T/ipykernel_2304/2047657456.py:12: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  table_caption = soup.find('caption', text='Per Game Table')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data has been exported to top_three_players_by_team.csv\n"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "\n",
    "for team_abbrev in teams:\n",
    "    search_url = f'https://www.basketball-reference.com/teams/{team_abbrev}/2023.html'\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the table with the specified caption\n",
    "        table_caption = soup.find('caption', text='Per Game Table')\n",
    "\n",
    "        if table_caption:\n",
    "            # Get the parent table element\n",
    "            table = table_caption.find_parent('table')\n",
    "\n",
    "            # Extract data from the thead\n",
    "            headers = [header.text.strip() for header in table.find('thead').find_all(['th'])]\n",
    "\n",
    "            # Extract data from the tbody\n",
    "            rows = []\n",
    "            for row in table.find('tbody').find_all('tr')[:3]:  # Collect the first three entries\n",
    "                player_column = row.find('td', class_='left')  # Assuming the class of the player column is 'left'\n",
    "                player_link = player_column.find('a')['href'] if player_column and player_column.find('a') else None\n",
    "                row_data = [data.text.strip() for data in row.find_all(['td', 'th'])]\n",
    "                rows.append({'team': team_abbrev, 'player_link': player_link, 'data': row_data})\n",
    "\n",
    "            # Add the rows to the list of all rows\n",
    "            all_rows.extend(rows)\n",
    "\n",
    "        else:\n",
    "            print(\"Table caption not found.\")\n",
    "    else:\n",
    "        print(\"Failed to fetch the webpage. Status code:\", response.status_code)\n",
    "\n",
    "# Save all data to a single CSV file\n",
    "csv_filename = 'top_three_players_by_team.csv'\n",
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header\n",
    "    csv_writer.writerow(['team'] + ['player_link'] + headers)\n",
    "\n",
    "    # Write the data rows\n",
    "    for row in all_rows:\n",
    "        csv_writer.writerow([row['team']]+[row['player_link']] + row['data'])\n",
    "\n",
    "print(f\"All data has been exported to {csv_filename}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "csv_filename = 'top_three_players_by_team.csv'\n",
    "\n",
    "# Initialize empty lists to store teams and player_links\n",
    "teams = []\n",
    "player_links = []\n",
    "\n",
    "with open(csv_filename, 'r', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "\n",
    "    # Read the header to get the index of 'team' and 'player_link' columns\n",
    "    header = next(csv_reader)\n",
    "    team_index = header.index('team') if 'team' in header else None\n",
    "    player_link_index = header.index('player_link') if 'player_link' in header else None\n",
    "\n",
    "    if team_index is not None and player_link_index is not None:\n",
    "        # Iterate through the rows\n",
    "        for row in csv_reader:\n",
    "            # Extract 'team' and 'player_link' from each row\n",
    "            team = row[team_index] if team_index < len(row) else None\n",
    "            player_link = row[player_link_index] if player_link_index < len(row) else None\n",
    "\n",
    "            # Append to the lists\n",
    "            teams.append(team)\n",
    "            player_links.append(player_link)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch the webpage. Status code: 429\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.basketball-reference.com\"\n",
    "\n",
    "all_player_rows = []\n",
    "\n",
    "delay_between_requests = 2  # Set a delay in seconds\n",
    "\n",
    "runs = 1\n",
    "# Print or use the extracted teams and player_links as needed\n",
    "for team, player_link in zip(teams, player_links):\n",
    "\n",
    "    runs -= 1\n",
    "\n",
    "    if runs < 0:\n",
    "        break\n",
    "\n",
    "    player_url = base_url + player_link.strip(\".html\") + \"/gamelog/2023\"\n",
    "\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.59 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 YaBrowser/21.6.2.855 Yowser/2.5 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 OPR/77.0.4054.275 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Whale/2.10.122.23 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Brave/91.1.25.68 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Vivaldi/4.0.2312.33 Safari/537.36'\n",
    "    ]\n",
    "\n",
    "\n",
    "    headers = {'User-Agent': random.choice(user_agents)}\n",
    "\n",
    "\n",
    "    response = requests.get(player_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the table with the specified caption\n",
    "        table_caption = soup.find('caption', text='2022-23 Regular Season Table')\n",
    "\n",
    "        if table_caption:\n",
    "            # Get the parent table element\n",
    "            table = table_caption.find_parent('table')\n",
    "\n",
    "            # Extract data from the thead\n",
    "            headers = [header.text.strip() for header in table.find('thead').find_all('th')]\n",
    "\n",
    "            # Extract data from the tbody\n",
    "            rows = []\n",
    "            table_rows = table.find('tbody').find_all('tr')\n",
    "            for row in table_rows:\n",
    "                row_class = row.get('class', [])\n",
    "                if 'thead' not in row_class:\n",
    "                    row_data = [data.text.strip() for data in row.find_all(['td', 'th'])]  # Include 'th' elements as well\n",
    "                    rows.append({'team': team_abbrev, 'data': row_data})\n",
    "\n",
    "            # Add the rows to the list of all rows\n",
    "            all_player_rows.extend(rows)\n",
    "\n",
    "        else:\n",
    "            print(\"Table caption not found.\")\n",
    "    else:\n",
    "        print(\"Failed to fetch the webpage. Status code:\", response.status_code)\n",
    "\n",
    "    time.sleep(delay_between_requests)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table caption not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/t55f5t150hv1sqqwhrm_md340000gn/T/ipykernel_2304/3551388406.py:5: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  table_caption = soup.find('caption', text='2022-23 Regular Season Table')\n"
     ]
    }
   ],
   "source": [
    "# Save all data to a single CSV file\n",
    "csv_filename = 'player_season_data_by_team.csv'\n",
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header\n",
    "    csv_writer.writerow(['team'] + headers)\n",
    "\n",
    "    # Write the data rows\n",
    "    for row in all_rows:\n",
    "        csv_writer.writerow([row['team']]+ row['data'])\n",
    "\n",
    "print(f\"All data has been exported to {csv_filename}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
