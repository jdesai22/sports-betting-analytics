{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# get top3 players from each team\n",
    "\n",
    "teams = ['ATL', 'BOS', 'BRK', 'CHI', 'CHO', 'CLE', 'DAL', 'DEN', 'DET', 'GSW', 'HOU', 'IND', 'LAC', 'LAL', 'MEM', 'MIA', 'MIL', 'MIN', 'NOP', 'NYK', 'OKC', 'ORL', 'PHI', 'PHO', 'POR', 'SAC', 'SAS', 'TOR', 'UTA', 'WAS']\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/t55f5t150hv1sqqwhrm_md340000gn/T/ipykernel_2304/2047657456.py:12: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  table_caption = soup.find('caption', text='Per Game Table')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data has been exported to top_three_players_by_team.csv\n"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "\n",
    "for team_abbrev in teams:\n",
    "    search_url = f'https://www.basketball-reference.com/teams/{team_abbrev}/2023.html'\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the table with the specified caption\n",
    "        table_caption = soup.find('caption', text='Per Game Table')\n",
    "\n",
    "        if table_caption:\n",
    "            # Get the parent table element\n",
    "            table = table_caption.find_parent('table')\n",
    "\n",
    "            # Extract data from the thead\n",
    "            headers = [header.text.strip() for header in table.find('thead').find_all(['th'])]\n",
    "\n",
    "            # Extract data from the tbody\n",
    "            rows = []\n",
    "            for row in table.find('tbody').find_all('tr')[:3]:  # Collect the first three entries\n",
    "                player_column = row.find('td', class_='left')  # Assuming the class of the player column is 'left'\n",
    "                player_link = player_column.find('a')['href'] if player_column and player_column.find('a') else None\n",
    "                row_data = [data.text.strip() for data in row.find_all(['td', 'th'])]\n",
    "                rows.append({'team': team_abbrev, 'player_link': player_link, 'data': row_data})\n",
    "\n",
    "            # Add the rows to the list of all rows\n",
    "            all_rows.extend(rows)\n",
    "\n",
    "        else:\n",
    "            print(\"Table caption not found.\")\n",
    "    else:\n",
    "        print(\"Failed to fetch the webpage. Status code:\", response.status_code)\n",
    "\n",
    "# Save all data to a single CSV file\n",
    "csv_filename = 'top_three_players_by_team.csv'\n",
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header\n",
    "    csv_writer.writerow(['team'] + ['player_link'] + headers)\n",
    "\n",
    "    # Write the data rows\n",
    "    for row in all_rows:\n",
    "        csv_writer.writerow([row['team']]+[row['player_link']] + row['data'])\n",
    "\n",
    "print(f\"All data has been exported to {csv_filename}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "csv_filename = 'top_three_players_by_team.csv'\n",
    "\n",
    "# Initialize empty lists to store teams and player_links\n",
    "teams = []\n",
    "player_links = []\n",
    "player_names = []\n",
    "\n",
    "with open(csv_filename, 'r', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "\n",
    "    # Read the header to get the index of 'team' and 'player_link' columns\n",
    "    header = next(csv_reader)\n",
    "    team_index = header.index('team') if 'team' in header else None\n",
    "    player_link_index = header.index('player_link') if 'player_link' in header else None\n",
    "    player_name_index = header.index('Player') if 'Player' in header else None\n",
    "\n",
    "    if team_index is not None and player_link_index is not None and player_name_index is not None:\n",
    "        # Iterate through the rows\n",
    "        for row in csv_reader:\n",
    "            # Extract 'team' and 'player_link' from each row\n",
    "            team = row[team_index] if team_index < len(row) else None\n",
    "            player_link = row[player_link_index] if player_link_index < len(row) else None\n",
    "            player_name = row[player_name_index] if player_name_index < len(row) else None\n",
    "\n",
    "            # Append to the lists\n",
    "            teams.append(team)\n",
    "            player_links.append(player_link)\n",
    "            player_names.append(player_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dejounte Murray\n",
      "Trae Young\n",
      "De'Andre Hunter\n",
      "Jayson Tatum\n",
      "Jaylen Brown\n",
      "Marcus Smart\n",
      "Kyrie Irving\n",
      "Kevin Durant\n",
      "Spencer Dinwiddie\n",
      "DeMar DeRozan\n",
      "Zach LaVine\n",
      "Nikola Vučević\n",
      "Terry Rozier\n",
      "LaMelo Ball\n",
      "P.J. Washington\n",
      "Donovan Mitchell\n",
      "Darius Garland\n",
      "Evan Mobley\n",
      "Kyrie Irving\n",
      "Luka Dončić\n",
      "Spencer Dinwiddie\n",
      "Nikola Jokić\n",
      "Jamal Murray\n",
      "Kentavious Caldwell-Pope\n",
      "Cade Cunningham\n",
      "Bojan Bogdanović\n",
      "Jaden Ivey\n",
      "Stephen Curry\n",
      "Klay Thompson\n",
      "Andrew Wiggins\n",
      "Kevin Porter Jr.\n",
      "Jalen Green\n",
      "Jabari Smith Jr.\n",
      "Tyrese Haliburton\n",
      "Buddy Hield\n",
      "Myles Turner\n",
      "Paul George\n",
      "Kawhi Leonard\n",
      "Russell Westbrook\n",
      "LeBron James\n",
      "Anthony Davis\n",
      "D'Angelo Russell\n",
      "Jacob Gilyard\n",
      "Ja Morant\n",
      "Desmond Bane\n",
      "Tyler Herro\n",
      "Bam Adebayo\n",
      "Jimmy Butler\n",
      "Jrue Holiday\n",
      "Giannis Antetokounmpo\n",
      "Brook Lopez\n",
      "Anthony Edwards\n",
      "Karl-Anthony Towns\n",
      "D'Angelo Russell\n",
      "CJ McCollum\n",
      "Brandon Ingram\n",
      "Zion Williamson\n",
      "Julius Randle\n",
      "Jalen Brunson\n",
      "RJ Barrett\n",
      "Shai Gilgeous-Alexander\n",
      "Josh Giddey\n",
      "Luguentz Dort\n",
      "Paolo Banchero\n",
      "Franz Wagner\n",
      "Wendell Carter Jr.\n",
      "James Harden\n",
      "Joel Embiid\n",
      "Tyrese Maxey\n",
      "Mikal Bridges\n",
      "Devin Booker\n",
      "Kevin Durant\n",
      "Damian Lillard\n",
      "Jerami Grant\n",
      "Anfernee Simons\n",
      "Domantas Sabonis\n",
      "De'Aaron Fox\n",
      "Harrison Barnes\n",
      "Keldon Johnson\n",
      "Devin Vassell\n",
      "Tre Jones\n",
      "Pascal Siakam\n",
      "Fred VanVleet\n",
      "OG Anunoby\n",
      "Lauri Markkanen\n",
      "Jordan Clarkson\n",
      "Mike Conley\n",
      "Kyle Kuzma\n",
      "Bradley Beal\n",
      "Kristaps Porziņģis\n"
     ]
    }
   ],
   "source": [
    "for team, player_link, player_name in zip(teams, player_links, player_names):\n",
    "    print(player_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/t55f5t150hv1sqqwhrm_md340000gn/T/ipykernel_2304/1843429476.py:43: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  table_caption = soup.find('caption', text='2022-23 Regular Season Table')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.basketball-reference.com\"\n",
    "\n",
    "all_player_rows = []\n",
    "\n",
    "delay_between_requests = 5.01  # Set a delay in seconds\n",
    "\n",
    "runs = 0\n",
    "# Print or use the extracted teams and player_links as needed\n",
    "for team, player_link, player_name in zip(teams, player_links, player_names):\n",
    "\n",
    "    runs += 1\n",
    "\n",
    "    print(runs)\n",
    "\n",
    "    # if runs < 0:\n",
    "    #     break\n",
    "\n",
    "    player_url = base_url + player_link.strip(\".html\") + \"/gamelog/2023\"\n",
    "\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.59 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 YaBrowser/21.6.2.855 Yowser/2.5 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 OPR/77.0.4054.275 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Whale/2.10.122.23 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Brave/91.1.25.68 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Vivaldi/4.0.2312.33 Safari/537.36'\n",
    "    ]\n",
    "\n",
    "\n",
    "    headers = {'User-Agent': random.choice(user_agents)}\n",
    "\n",
    "\n",
    "    response = requests.get(player_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the table with the specified caption\n",
    "        table_caption = soup.find('caption', text='2022-23 Regular Season Table')\n",
    "\n",
    "        if table_caption:\n",
    "            # Get the parent table element\n",
    "            table = table_caption.find_parent('table')\n",
    "\n",
    "            # Extract data from the thead\n",
    "            headers = [header.text.strip() for header in table.find('thead').find_all('th')]\n",
    "\n",
    "            # Extract data from the tbody\n",
    "            rows = []\n",
    "            table_rows = table.find('tbody').find_all('tr')\n",
    "            for row in table_rows:\n",
    "                row_class = row.get('class', [])\n",
    "                if 'thead' not in row_class:\n",
    "                    row_data = [data.text.strip() for data in row.find_all(['td', 'th'])]  # Include 'th' elements as well\n",
    "                    rows.append({'Player': player_name, 'team': team, 'data': row_data})\n",
    "\n",
    "            # Add the rows to the list of all rows\n",
    "            all_player_rows.extend(rows)\n",
    "\n",
    "        else:\n",
    "            print(\"Table caption not found.\")\n",
    "    else:\n",
    "        if response.status_code == 429:\n",
    "            print(\"Failed to fetch the webpage. Status code:\", response.status_code)\n",
    "            break\n",
    "        print(\"Failed to fetch the webpage. Status code:\", response.status_code)\n",
    "\n",
    "    time.sleep(delay_between_requests)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data has been exported to player_season_data_by_team.csv\n"
     ]
    }
   ],
   "source": [
    "# Save all data to a single CSV file\n",
    "csv_filename = 'player_season_data_by_team.csv'\n",
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header\n",
    "    csv_writer.writerow(['Player'] + ['team'] + headers)\n",
    "\n",
    "    # Write the data rows\n",
    "    for row in all_player_rows:\n",
    "        csv_writer.writerow([row['Player']]+[row['team']]+ row['data'])\n",
    "\n",
    "print(f\"All data has been exported to {csv_filename}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATL\n",
      "/players/y/youngtr01.html\n"
     ]
    }
   ],
   "source": [
    "print(team)\n",
    "print(player_link)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Input and output file paths\n",
    "input_csv_file = 'top_three_players_by_team.csv'  # Replace with the actual input CSV file\n",
    "output_csv_file = 'top_three_players_by_team_with_pos.csv'  # Replace with the desired output CSV file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/t55f5t150hv1sqqwhrm_md340000gn/T/ipykernel_7249/266481060.py:44: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  table_caption = soup.find('caption', text='Per Game Table')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG\n",
      "2\n",
      "PG\n",
      "3\n",
      "SF\n",
      "4\n",
      "SF\n",
      "5\n",
      "SF\n",
      "6\n",
      "PG\n",
      "7\n",
      "PG,SG\n",
      "8\n",
      "PF\n",
      "9\n",
      "PG,SG\n",
      "10\n",
      "SF\n",
      "11\n",
      "SG\n",
      "12\n",
      "C\n",
      "13\n",
      "SG\n",
      "14\n",
      "PG\n",
      "15\n",
      "PF\n",
      "16\n",
      "SG\n",
      "17\n",
      "PG\n",
      "18\n",
      "PF\n",
      "19\n",
      "PG,SG\n",
      "20\n",
      "PG\n",
      "21\n",
      "PG,SG\n",
      "22\n",
      "C\n",
      "23\n",
      "PG\n",
      "24\n",
      "SG\n",
      "25\n",
      "PG\n",
      "26\n",
      "PF\n",
      "27\n",
      "SG\n",
      "28\n",
      "PG\n",
      "29\n",
      "SF\n",
      "30\n",
      "SF\n",
      "31\n",
      "PG\n",
      "32\n",
      "SG\n",
      "33\n",
      "PF\n",
      "34\n",
      "PG\n",
      "35\n",
      "SF\n",
      "36\n",
      "C\n",
      "37\n",
      "SF\n",
      "38\n",
      "SF\n",
      "39\n",
      "PG\n",
      "40\n",
      "PF\n",
      "41\n",
      "C\n",
      "42\n",
      "PG\n",
      "43\n",
      "PG\n",
      "44\n",
      "PG\n",
      "45\n",
      "SG\n",
      "46\n",
      "SG\n",
      "47\n",
      "C\n",
      "48\n",
      "PF\n",
      "49\n",
      "PG\n",
      "50\n",
      "PF\n",
      "51\n",
      "C\n",
      "52\n",
      "SG\n",
      "53\n",
      "PF\n",
      "54\n",
      "PG\n",
      "55\n",
      "PG\n",
      "56\n",
      "SF\n",
      "57\n",
      "PF\n",
      "58\n",
      "PF\n",
      "59\n",
      "PG\n",
      "60\n",
      "SG\n",
      "61\n",
      "PG\n",
      "62\n",
      "PF\n",
      "63\n",
      "SF\n",
      "64\n",
      "PF\n",
      "65\n",
      "SF\n",
      "66\n",
      "C\n",
      "67\n",
      "PG\n",
      "68\n",
      "C\n",
      "69\n",
      "SG\n",
      "70\n",
      "SF,SG\n",
      "71\n",
      "SG\n",
      "72\n",
      "PF\n",
      "73\n",
      "PG\n",
      "74\n",
      "PF\n",
      "75\n",
      "SG\n",
      "76\n",
      "C\n",
      "77\n",
      "PG\n",
      "78\n",
      "PF\n",
      "79\n",
      "SF\n",
      "80\n",
      "SG\n",
      "81\n",
      "PG\n",
      "82\n",
      "PF\n",
      "83\n",
      "PG\n",
      "84\n",
      "SF\n",
      "85\n",
      "PF\n",
      "86\n",
      "SG\n",
      "87\n",
      "PG\n",
      "88\n",
      "PF\n",
      "89\n",
      "SG\n",
      "90\n",
      "C\n"
     ]
    }
   ],
   "source": [
    "# Read data from the input CSV file and update the rows with player positions\n",
    "with open(input_csv_file, 'r') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    rows = list(reader)\n",
    "\n",
    "    runs = 0\n",
    "\n",
    "    for row in rows:\n",
    "        time.sleep(5.01)\n",
    "        runs += 1\n",
    "        print(runs)\n",
    "        # if runs > 1:\n",
    "        #     break\n",
    "\n",
    "        player_link = row.get('player_link')\n",
    "        position = None\n",
    "\n",
    "        url = f'https://www.basketball-reference.com{player_link}'  # Replace 'www.example.com' with the actual domain\n",
    "\n",
    "        user_agents = [\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.59 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 YaBrowser/21.6.2.855 Yowser/2.5 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 OPR/77.0.4054.275 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Whale/2.10.122.23 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Brave/91.1.25.68 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Vivaldi/4.0.2312.33 Safari/537.36'\n",
    "        ]\n",
    "\n",
    "\n",
    "        headers = {'User-Agent': random.choice(user_agents)}\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find the table with the specified caption\n",
    "            table_caption = soup.find('caption', text='Per Game Table')\n",
    "\n",
    "            if table_caption:\n",
    "                # Get the parent table element\n",
    "                per_game_table = table_caption.find_parent('table')\n",
    "\n",
    "                # Find the table row with id=\"per_game.2023\"\n",
    "                tr = per_game_table.find('tr', {'id': 'per_game.2023'})\n",
    "\n",
    "                if tr:\n",
    "                    # Find the td (table data) with the attribute data-stat=\"pos\" within the tr\n",
    "                    td_with_pos = tr.find('td', {'data-stat': 'pos'})\n",
    "\n",
    "                    if td_with_pos:\n",
    "                        # Extract the text content from the td\n",
    "                        position = td_with_pos.text.strip()\n",
    "        else:\n",
    "            print(response.status_code)\n",
    "            break\n",
    "\n",
    "        row['Position'] = position\n",
    "        print(position)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Write the updated data with the new 'Position' column to a new CSV file\n",
    "fieldnames = reader.fieldnames + ['Position']\n",
    "with open(output_csv_file, 'w', newline='') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/t55f5t150hv1sqqwhrm_md340000gn/T/ipykernel_27645/2035458946.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Assuming your CSV file is named 'nba_data.csv'\n",
    "df = pd.read_csv('player_season_data_by_team.csv')\n",
    "\n",
    "# Remove rows where 'GS' column has value 'Inactive'\n",
    "df = df[df['GS'] != 'Inactive']\n",
    "\n",
    "# Save the modified dataframe to a new CSV file or overwrite the existing one\n",
    "df.to_csv('player_season_data_by_team_removed_inactive.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('player_season_data_by_team_removed_inactive.csv')\n",
    "\n",
    "# Calculate running averages for each player and round to 3 decimal places\n",
    "df['Points_Running_Avg'] = (df.groupby('Player')['PTS'].cumsum() / (df.groupby('Player')['PTS'].cumcount() + 1)).round(3)\n",
    "df['Rebounds_Running_Avg'] = (df.groupby('Player')['TRB'].cumsum() / (df.groupby('Player')['TRB'].cumcount() + 1)).round(3)\n",
    "df['Assists_Running_Avg'] = (df.groupby('Player')['AST'].cumsum() / (df.groupby('Player')['AST'].cumcount() + 1)).round(3)\n",
    "df['Steals_Running_Avg'] = (df.groupby('Player')['STL'].cumsum() / (df.groupby('Player')['STL'].cumcount() + 1)).round(3)\n",
    "df['Blocks_Running_Avg'] = (df.groupby('Player')['BLK'].cumsum() / (df.groupby('Player')['BLK'].cumcount() + 1)).round(3)\n",
    "\n",
    "# Calculate differences for each player and round to 3 decimal places\n",
    "df['Points_Diff'] = (df['PTS'] - df['Points_Running_Avg']).round(3)\n",
    "df['Rebounds_Diff'] = (df['TRB'] - df['Rebounds_Running_Avg']).round(3)\n",
    "df['Assists_Diff'] = (df['AST'] - df['Assists_Running_Avg']).round(3)\n",
    "df['Steals_Diff'] = (df['STL'] - df['Steals_Running_Avg']).round(3)\n",
    "df['Blocks_Diff'] = (df['BLK'] - df['Blocks_Running_Avg']).round(3)\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "df.to_csv('annotated_player_data.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Assuming your CSV files are named 'player_data.csv' and 'top_three_players.csv'\n",
    "player_data = pd.read_csv('annotated_player_data.csv')\n",
    "top_three_players = pd.read_csv('top_three_players_by_team_with_pos.csv')\n",
    "\n",
    "# Merge DataFrames based on the 'Player' column\n",
    "merged_df = pd.merge(player_data, top_three_players[['Player', 'Position']], on='Player', how='left')\n",
    "\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file or overwrite the existing one\n",
    "merged_df.to_csv('final_annotated_player_data.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
